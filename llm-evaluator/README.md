# LLM Evaluator

ローカル大規模言語モデル（LLM）を体系的に評価・比較するための包括的なWebアプリケーションです。標準化された質問と日本語に特化した評価基準を使用してLLMの性能を効率的にテストできます。

## 機能

- **LLMモデル管理**: ローカルLLMエンドポイントの登録・管理
- **質問バンク**: 評価用質問の作成・整理
- **構造化評価**: 5つの評価項目（正確性、網羅性、論理構成、日本語、総合）による評価
- **結果分析**: フィルタリング機能付きの詳細結果表示
- **分析ダッシュボード**: パフォーマンス比較と統計情報
- **日本語サポート**: 日本語UIと日本語最適化された評価基準

## クイックスタート

### 前提条件

- Node.js（Next.js 15.4.2と互換性のあるバージョン）
- OpenAI互換APIを持つローカルLLM（例：Ollama、LM Studio）

### インストール

```bash
# プロジェクトディレクトリに移動
cd llm-evaluator

# 依存関係をインストール
npm install

# 開発サーバーを起動
npm run dev
```

[http://localhost:3000](http://localhost:3000) でアプリケーションにアクセスできます。

## 使用方法

### 1. LLMモデルの設定

評価を開始する前に、ローカルLLMモデルを登録します：

1. ダッシュボードから **LLMモデル管理** に移動
2. **新規追加** をクリック
3. モデルの詳細を入力：
   - **名前**: モデルの分かりやすい名前
   - **エンドポイント**: API URL（例：`http://localhost:11434/v1/chat/completions`）
   - **APIキー**: 認証が必要な場合（任意）
   - **説明**: モデルに関するメモ（任意）
4. モデルを保存

**一般的なエンドポイント:**
- **Ollama**: `http://localhost:11434/v1/chat/completions`
- **LM Studio**: `http://localhost:1234/v1/chat/completions`
- **カスタム**: その他のOpenAI互換エンドポイント

### 2. 質問の作成

評価用の質問バンクを構築します：

1. **質問管理** に移動
2. **新規追加** をクリック
3. 以下を入力：
   - **タイトル**: 質問の簡潔な識別子
   - **内容**: 完全な質問文
   - **カテゴリ**: グループ化用（例：「コーディング」「推論」）（任意）
4. 質問を保存

### 3. 評価の実行

モデル評価を実行します：

1. **評価実行** にアクセス
2. ドロップダウンから **LLMモデル** を選択
3. ドロップダウンから **質問** を選択
4. **LLMの回答を取得** をクリック
5. モデルの回答を確認
6. 5つの評価項目で1-5スケールで評価：
   - **正確性**: 事実の正確さ
   - **網羅性**: トピックのカバー範囲
   - **論理構成**: 論理的構造と流れ
   - **日本語**: 日本語の品質
   - **総合**: 総合的な評価
7. 任意で **評価コメント** を追加
8. **評価を保存** をクリック

### 4. 結果の表示

評価結果を分析します：

1. **評価結果** にアクセス
2. フィルターを使用して結果を絞り込み：
   - **モデル** でフィルター
   - **質問** でフィルター
3. 色分けされたスコアを確認：
   - 🟢 緑: スコア ≥ 4（良好）
   - 🟡 黄: スコア ≥ 3（平均的）
   - 🔴 赤: スコア < 3（要改善）
4. 詳細な回答とスコアを表示

### 5. 統計・分析

パフォーマンスの洞察を取得：

1. **統計・分析** を開く
2. 概要統計を確認：
   - 実施済み評価数
   - テスト済みモデル数
   - 質問バンク内の質問数
3. 棒グラフでモデルパフォーマンスを比較
4. 難しい質問を特定（平均スコアが低い）
5. トップパフォーマンスモデルを追跡

## データ構造

アプリケーションはシンプルなJSONファイルストレージを使用：

- **models.json**: LLMモデル設定
- **questions.json**: 評価質問
- **evaluations.json**: 評価結果とスコア

すべてのデータは自動的に管理され、ローカルに保存されます。

## 評価システム

各回答は5つの評価項目について1-5スケールで評価されます：

| スコア | 説明 |
|--------|------|
| 5 | 優秀 |
| 4 | 良好 |
| 3 | 平均的 |
| 2 | 平均以下 |
| 1 | 不良 |

## APIエンドポイント

アプリケーションはすべての操作に対してREST APIを提供：

- **Models**: `/api/models`（CRUD操作）
- **Questions**: `/api/questions`（CRUD操作）
- **Evaluations**: `/api/evaluations`（作成、読み取り）
- **LLM Chat**: `/api/llm/chat`（モデル通信）

## 開発

### 利用可能なスクリプト

```bash
# 開発サーバー
npm run dev

# プロダクションビルド
npm run build

# プロダクションサーバー開始
npm start

# リント実行
npm run lint
```

### プロジェクト構造

```
src/
├── app/                    # Next.js App Routerページ
│   ├── models/            # モデル管理
│   ├── questions/         # 質問管理
│   ├── evaluation/        # 評価実行
│   ├── results/           # 結果表示
│   ├── analytics/         # 分析ダッシュボード
│   └── api/              # APIルート
├── lib/                   # データ管理ユーティリティ
├── types/                 # TypeScript型定義
└── data/                  # JSONデータストレージ
```

## トラブルシューティング

### よくある問題

1. **LLM接続エラー**: ローカルLLMが実行中でアクセス可能であることを確認
2. **APIキーの問題**: モデルが認証を必要とするかどうかを確認
3. **ポート競合**: ポート3000が利用可能であることを確認、または `npm run dev -- -p 3001` でポートを変更

### ローカルLLMセットアップ例

**Ollama:**
```bash
# Ollamaをインストールして実行
ollama serve
ollama run llama2  # または好みのモデル
```

**LM Studio:**
- LM Studioをダウンロードしてインストール
- モデルをロードしてローカルサーバーを開始
- 提供されるエンドポイントURLをメモ

## ライセンス

このプロジェクトはオープンソースです。詳細はLICENSEファイルを参照してください。

## 貢献

貢献を歓迎します！イシューやプルリクエストをお気軽に送信してください。

---

[Next.js](https://nextjs.org)、TypeScript、Tailwind CSSで構築されています。
